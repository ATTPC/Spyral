{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining Point Clouds\n",
    "\n",
    "This notebook is for examining the point cloud data generated by the first analysis phase (phase pointcloud). You do not need to have run Spyral to run this notebook, you simply must have created a valid Spyral input configuration. Plots of the individual traces point clouds in two and three dimensions can be made to check the status of the results. This is helpful for testing the parameters generated by the first phase. It actually runs a mini-version of phase 1. Note that the data generated here is NOT saved. This is only for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "First we import all of the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyral.core.point_cloud import PointCloud\n",
    "from spyral.core.run_stacks import form_run_string\n",
    "from spyral.trace.get_event import GetEvent, GET_DATA_TRACE_START, GET_DATA_TRACE_STOP\n",
    "from spyral.trace.get_legacy_event import GetLegacyEvent\n",
    "from spyral.phases.pointcloud_phase import get_event_range\n",
    "from spyral.correction import create_electron_corrector\n",
    "from spyral.core.pad_map import PadMap\n",
    "\n",
    "from spyral import PadParameters, GetParameters, FribParameters, DetectorParameters, INVALID_PATH, PointcloudPhase, PointcloudLegacyPhase\n",
    "\n",
    "import h5py as h5\n",
    "import numpy.random as random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def find_trace_from_padid(event: GetEvent | GetLegacyEvent, pad_id: int) -> int:\n",
    "    for idx, trace in enumerate(event.traces):\n",
    "        if trace.hw_id.pad_id == pad_id:\n",
    "            return idx\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Now we'll load the configuration we want to use. Configurations are stored in JSON files using the conventions defined by the example file shipped with the repository (config.json). By default, this notebook reads the example file, but this can be changed. Additionally, once the config is loaded, you can always tweak the fields for rapid testing of different parameters.\n",
    "\n",
    "We then hand off the workspace configuration to the Workspace class. The Workspace helps us handle paths to various files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "trace_path = Path(\"/path/to/raw/attpc/traces/\")\n",
    "workspace_path = Path(\"/path/to/your/workspace/\")\n",
    "\n",
    "# Are we analyzing legacy data?\n",
    "is_legacy: bool = False\n",
    "\n",
    "# Pad mapping. We use defaults here\n",
    "pad_params = PadParameters(\n",
    "    is_default=True,\n",
    "    is_default_legacy=False,\n",
    "    pad_geometry_path=INVALID_PATH,\n",
    "    pad_gain_path=INVALID_PATH,\n",
    "    pad_time_path=INVALID_PATH,\n",
    "    pad_electronics_path=INVALID_PATH,\n",
    "    pad_scale_path=INVALID_PATH,\n",
    ")\n",
    "\n",
    "# AT-TPC GET trace analysis\n",
    "get_params = GetParameters(\n",
    "    baseline_window_scale=20.0,\n",
    "    peak_separation=50.0,\n",
    "    peak_prominence=20.0,\n",
    "    peak_max_width=50.0,\n",
    "    peak_threshold=40.0,\n",
    ")\n",
    "\n",
    "# AT-TPC FRIBDAQ trace analysis (or legacy GET extension)\n",
    "frib_params = FribParameters(\n",
    "    baseline_window_scale=100.0,\n",
    "    peak_separation=50.0,\n",
    "    peak_prominence=20.0,\n",
    "    peak_max_width=500.0,\n",
    "    peak_threshold=100.0,\n",
    "    ic_delay_time_bucket=1100,\n",
    "    ic_multiplicity=1,\n",
    "    correct_ic_time=True,\n",
    ")\n",
    "\n",
    "# Detector properties\n",
    "det_params = DetectorParameters(\n",
    "    magnetic_field=2.85,\n",
    "    electric_field=45000.0,\n",
    "    detector_length=1000.0,\n",
    "    beam_region_radius=25.0,\n",
    "    micromegas_time_bucket=10.0,\n",
    "    window_time_bucket=560.0,\n",
    "    get_frequency=6.25,\n",
    "    garfield_file_path=Path(\"/path/to/some/garfield.txt\"),\n",
    "    do_garfield_correction=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Now that our configuration is loaded, we can start reading and analyzing some data. Step one is to access the raw trace datafile. This means that you need to pick a run to analyze; we store the run number in a variable for later reference. To analyze a different run simply change the run number.\n",
    "\n",
    "We then ask the workspace to retrieve the trace file path for our selected run number. We then use the h5py library to open the associated h5 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = 16 # pick a run\n",
    "trace_file_path = trace_path / f\"{form_run_string(run_number)}.h5\"\n",
    "trace_file = h5.File(trace_file_path, \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our file is now loaded. Now we need to navigate to the correct group of the h5 structure. The traces are stored in the 'get' group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_group: h5.Group = trace_file['get']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a group loaded, we can now access the actual trace data! The default loading behavior is to select a random event (set of traces), since when testing you'll want to test your parameters on many different events. However, one can also fix the event by setting the event number to a constant value for debugging. Once the event number is chosen we then retrieve the raw trace data from the h5 file as a Dataset. Note that this operation can fail sometimes, if an event was omitted for some reason. If that happens just select a different event number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the trace file for the range of events\n",
    "min_event, max_event = get_event_range(trace_file)\n",
    "rng = random.default_rng()\n",
    "# Select a random event\n",
    "event_number = rng.integers(min_event, max_event)\n",
    "# Can always overwrite with hardcoded event number if needed\n",
    "# event_number = 2941\n",
    "print(f'Event: {event_number}')\n",
    "\n",
    "event_data: h5.Dataset = trace_group[f'evt{event_number}_data']\n",
    "event = None\n",
    "correction_path: Path\n",
    "# Load either a legacy or contemporary GET daq event, and create our assets\n",
    "if is_legacy:\n",
    "    event = GetLegacyEvent(event_data, event_number, get_params, frib_params, rng)\n",
    "    phase = PointcloudLegacyPhase(get_params, frib_params, det_params, pad_params)\n",
    "    phase.create_assets(workspace_path)\n",
    "    correction_path = phase.electron_correction_path\n",
    "else:\n",
    "    event = GetEvent(event_data, event_number, get_params, rng)\n",
    "    phase = PointcloudPhase(get_params, frib_params, det_params, pad_params)\n",
    "    phase.create_assets(workspace_path)\n",
    "    correction_path = phase.electron_correction_path\n",
    "\n",
    "pad_map = PadMap(pad_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing\n",
    "\n",
    "Now that we've loaded our data, it's time to do some analysis! Step one is just to see what some traces look like, without any analysis at all. Again here, we're going to pick a random trace to look at, but you may want to pick a specific trace depending on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_number = random.randint(0, len(event_data))\n",
    "# trace_number = find_trace_from_padid(event, 397)\n",
    "raw_trace_data = event_data[trace_number]\n",
    "time_bucket_range = np.arange(start=0, stop=512)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=time_bucket_range, y=raw_trace_data[GET_DATA_TRACE_START:GET_DATA_TRACE_STOP], mode=\"lines\", name=f\"Raw Trace {trace_number}\")\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=time_bucket_range, y=event.traces[trace_number].trace, mode=\"lines\",name=f\"Baseline Corrected Trace {trace_number}\")\n",
    ")\n",
    "print(f\"Trace Number: {trace_number}\")\n",
    "print(f\"Trace Hardware: {event.traces[trace_number].hw_id}\")\n",
    "peak_amps = []\n",
    "peak_cents = []\n",
    "peak_left = []\n",
    "peak_left_amps = []\n",
    "peak_right = []\n",
    "peak_right_amps = []\n",
    "for peak in event.traces[trace_number].get_peaks():\n",
    "    peak_amps.append(peak.amplitude)\n",
    "    peak_cents.append(np.floor(peak.centroid))\n",
    "    peak_left.append(peak.positive_inflection)\n",
    "    peak_right.append(peak.negative_inflection)\n",
    "    peak_left_amps.append(event.traces[trace_number].trace[int(peak.positive_inflection)])\n",
    "    peak_right_amps.append(event.traces[trace_number].trace[int(peak.negative_inflection)])\n",
    "print(f\"Peak centroids: {peak_cents}\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=peak_cents, y=peak_amps, mode=\"markers\", name=\"Peaks\")\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=peak_left, y=peak_left_amps, mode=\"markers\", name=\"Peak Left Edges\")\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=peak_right, y=peak_right_amps, mode=\"markers\", name=\"Peak Right Edges\")\n",
    ")\n",
    "fig.update_legends()\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time Bucket\",\n",
    "    yaxis_title=\"Amplitude\",\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you should see the plot of the raw trace as well as the baseline corrected trace. The baseline corrected trace is computed using a low-pass filter. This is done by passing the data to the GetEvent class which performs some signal analysis, identifying the peaks in the signal. The peaks are labeled with their centroids and left and right edges. To look at different traces, you can run the above cell over and over again; it will select a random trace in the event each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that peaks have been found, we can form a point cloud! Here we'll also apply the eletric field correction from Garfield. If you haven't run Spyral at all, this cell might take a bit to run, as the electric field correction will need to be generated if it is turned on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = PointCloud()\n",
    "\n",
    "# Do the electric field correction if requested\n",
    "corrector = None\n",
    "if correction_path.exists():\n",
    "    corrector = create_electron_corrector(correction_path)\n",
    "\n",
    "cloud.load_cloud_from_get_event(event, pad_map)\n",
    "hover_text = [f\"Pad ID: {int(point[5])}\" for point in cloud.cloud] # We'll use this later\n",
    "\n",
    "fig = make_subplots(2, 1, row_heights=[0.66, 0.33], specs=[[{\"type\": \"xy\"}], [{\"type\": \"scene\"}]])\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=cloud.cloud[:, 2], \n",
    "        y=cloud.cloud[:, 0], \n",
    "        z=cloud.cloud[:, 1], \n",
    "        mode=\"markers\",\n",
    "        text = hover_text,\n",
    "        hovertemplate=\"X: %{y:.2f}<br>Y: %{z:.2f}<br>Z: %{x:.2f}<br>%{text}\",\n",
    "        marker= {\n",
    "            \"size\": 3, \n",
    "            \"color\": cloud.cloud[:, 3], \n",
    "            \"showscale\": True\n",
    "            }, \n",
    "        name=\"Point Cloud\"\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cloud.cloud[:, 0], \n",
    "        y=cloud.cloud[:, 1], \n",
    "        mode=\"markers\",\n",
    "        text = hover_text,\n",
    "        hovertemplate=\"X: %{x:.2f}<br>Y: %{y:.2f}<br>%{text}\",\n",
    "        marker= {\n",
    "            \"color\": cloud.cloud[:, 3], \n",
    "            \"showscale\": True\n",
    "        }, \n",
    "        name=\"XY Projection\"),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title = \"X (mm)\",\n",
    "    yaxis_title = \"Y (mm)\",\n",
    "    xaxis_range=[-300.0, 300.0],\n",
    "    yaxis_range=[-300.0, 300.0],\n",
    "    scene = {\n",
    "        \"xaxis_title\": \"Z (Time Buckets)\",\n",
    "        \"yaxis_title\": \"X (mm)\",\n",
    "        \"zaxis_title\": \"Y (mm)\",\n",
    "        \"aspectratio\": {\n",
    "            \"x\": 3.3,\n",
    "            \"y\": 1.0,\n",
    "            \"z\": 1.0\n",
    "        },\n",
    "        \"xaxis_range\": [0.0, 512.0],\n",
    "        \"yaxis_range\": [-300.0, 300.0],\n",
    "        \"zaxis_range\": [-300.0, 300.0],\n",
    "    },\n",
    "    width = 1000,\n",
    "    height = 1500,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you should see your point cloud plotted in 3D as well as the X-Y plane (pad plane) projection. The marker color indicates the integral of the peak used to make the point in the point cloud. If you hover over one of the points in either plot, you'll see a label which shows the coordinate position as well as the trace and peak number which produced the point. This can be used to pick specific traces to examine!\n",
    "\n",
    "Notice that the z-axis is still in Time Buckets. We would like to convert this time axis into a position. To do this we use the reference time of the window and micromegas mesh (i.e. the position of the ends of the detector within the trigger). These values have to be estimated from the data. Typically this is handled by looking for window events (events where the beam reacted with the window), because they typically span the entire volume of the detector. Once you've set these values in your config, run the cell below to re-plot the point cloud with calibrated z-position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cloud.calibrate_z_position(det_params.micromegas_time_bucket, det_params.window_time_bucket, det_params.detector_length, efield_correction=corrector)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=cloud.cloud[:, 2], \n",
    "        y=cloud.cloud[:, 0], \n",
    "        z=cloud.cloud[:, 1], \n",
    "        mode=\"markers\", \n",
    "        marker= {\n",
    "            \"size\": 3, \n",
    "            \"color\": cloud.cloud[:, 4], \n",
    "            \"showscale\": True\n",
    "        }, \n",
    "        name=\"Point Cloud\"\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    scene = {\n",
    "        \"xaxis_range\": [0.0, 1000.0],\n",
    "        \"yaxis_range\": [-300.0, 300.0],\n",
    "        \"zaxis_range\": [-300.0, 300.0],\n",
    "        \"xaxis_title\": \"Z (mm)\",\n",
    "        \"yaxis_title\": \"X (mm)\",\n",
    "        \"zaxis_title\": \"Y (mm)\",\n",
    "        \"aspectratio\": {\n",
    "            \"x\": 3.3,\n",
    "            \"y\": 1.0,\n",
    "            \"z\": 1.0\n",
    "        }\n",
    "    },\n",
    "    height=750,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything should look more or less the same. You've now generated a point cloud! To examine a new event, you can simply re-run the entire notebook, which will select a new random event from the run of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still more you can look at however. You can even plot some interesting physics! Below is an example intended to try and plot a Bragg curve from the point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot r-Charge projection\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.linalg.norm(cloud.cloud[:, :3], axis=1), y=cloud.cloud[:, 4], mode=\"markers\", marker={\"size\": 5})\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Position (mm)\",\n",
    "    yaxis_title=\"Integral\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "That is a basic analysis of the traces and the point cloud data! With well tuned parameters, you're now ready to run the phase 1 analysis. Follow the instructions in the README to do this. Once thats done, you can move on to the next stage, generating and identifying clusters within the point clouds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
